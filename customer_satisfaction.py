# -*- coding: utf-8 -*-
"""Customer_satisfaction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fh2zHJWQOB2wRHhArS1O9QzV_-K6Rlm1

# **Importando Bibliotecas e iniciando Sessão no Spark**
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pyspark

from google.colab import drive
from pyspark.sql import SparkSession
from pyspark.sql import functions as f
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.types import StringType
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

spark = SparkSession.builder\
.master('local[*]')\
.appName('ClientSatisfaction')\
.getOrCreate()

drive.mount('/content/drive')

"""# **Análises Preliminares**"""

dados = spark.read.csv('/content/drive/MyDrive/.Data Science/Projetos para Portfólio/Santander/Customer Satisfaction/data/train.csv',
                       header=True, inferSchema=True)

dados.count()

len(dados.columns)

dados.show(truncate=False)

types = []
for tipo in dados.dtypes:
  types.append(tipo[1])
set(types)

"""O laço que criei está pegando o **tipo** de cada coluna e colocando em uma lista chamada **type**, depois converto a lista em **set** para remover as duplicatas e descobrir quais são os **tipos** presentes no DataFrame """

dados.show(truncate=False)

dados.select('ID').dtypes

"""Há um problema na coluna **ID**, está como ***int***, porém essa coluna é a **chave-primária** e por isso o ideal é que seja uma **string**, por isso irei mudar o tipo dessa coluna."""

dados = dados.withColumn('ID', dados['ID'].cast(StringType()))

dados.select('ID').dtypes

"""Agora que formato da coluna **ID** está correto. Irei procurar por dados nulos."""

dados.select(sum([f.count(f.when(f.isnan(c)|f.isnull(c), True)).alias(c)for c in dados.columns]).alias('Valores Nulos Totais')).show()

dados.groupBy('TARGET').count().show()

target = [3008, 73012] #Número de elementos da classe 1 e da classe 0 respectivamente
labels = ['Insatisfeito','Satisfeito'] # Nome das classes

ax = plt.figure(figsize=(12,8))
plt.pie(x=target, labels=labels)
plt.title('Comparando o tamanho das Classes', fontsize=20)
ax.show()

"""# **Preparando os dados para os modelos de Machine Learning**

### Preparando os dados para o modelo
"""

dados = dados.withColumnRenamed('TARGET', 'label')

dados.select('label').count()

"""## **Tratando o desbalanceamento com weightCol**

Como os classes estão **desbalanceadas**, vou adotar a abordagem de criar uma coluna com o peso das classes.

Os passos são:
1. Descobrir a **quantidade de elementos em cada classe** e o **total**; 
2. Usar a formula (n_total / (n_classes*n_elementos_na_classe)), isso para as duas classes;
3. Criar uma coluna com o valor do peso referente a classe.
"""

dados.select(f.count(f.when(dados['label']==1, True)).alias('n_class_1')).show()

dados.select(f.count(f.when(dados['label']==0, True)).alias('n_class_0')).show()

Class_0_weight = 76020/(2*73012)
Class_1_weight = 76020/(2*3008)

print(f'O peso da classe 0 é {Class_0_weight} \nO peso da classe 1 é {Class_1_weight}')

"""
A baixo sobrescrevo o DataFrame dados, adicionando uma coluna com os pessos
"""

dados = dados.select('*',(f.when(dados['label']==0, Class_0_weight).otherwise(Class_1_weight)).alias('ColWeight'))

dados.select('ColWeight').show()

dados.agg(f.countDistinct('ColWeight').alias('Valores Distintos')).show()

dados.select('ColWeight').distinct().show()

X = dados.columns
X.remove('label')
X.remove('ID')
X.remove('ColWeight')

assembler = VectorAssembler(inputCols=X, outputCol='features')

dados_preparados = assembler.transform(dados).select('features', 'label', 'ColWeight')

dados_preparados.show()

SEED = 101

train, test = dados_preparados.randomSplit([0.7, 0.3], seed=SEED)

train.count()

test.count()

"""# **Testando Modelos**
Agora chegou o momento de testar modelos de classificação e escolher um com as melhores métricas.

## Criando Funções para métricas
"""

def calcula_mostra_matriz_confusao(df_transform_modelo, normalize=False, percentage=True):
  tp = df_transform_modelo.select('label', 'prediction').where((f.col('label') == 1) & (f.col('prediction') == 1)).count()
  tn = df_transform_modelo.select('label', 'prediction').where((f.col('label') == 0) & (f.col('prediction') == 0)).count()
  fp = df_transform_modelo.select('label', 'prediction').where((f.col('label') == 0) & (f.col('prediction') == 1)).count()
  fn = df_transform_modelo.select('label', 'prediction').where((f.col('label') == 1) & (f.col('prediction') == 0)).count()

  valorP = 1
  valorN = 1

  if normalize:
    valorP = tp + fn
    valorN = fp + tn

  if percentage and normalize:
    valorP = valorP / 100
    valorN = valorN / 100

  print(' '*20, 'Previsto')
  print(' '*15, 'Atrito', ' '*5 ,'Não-Atrito')
  print(' '*4, 'Atrito', ' '*6, int(tp/valorP), ' '*7, int(fn/valorP))
  print('Real')
  print(' '*4, 'Não-Atrito', ' '*2, int(fp/valorN), ' '*7, int(tn/valorN))

def calcula_metricas(nome_modelo, previsao_treino, previsao_teste):
  print('='*50)
  print(nome_modelo)
  print('='*50)
  print('')
  print('-'*50)
  print('Dados de Treino')
  print('-'*50)
  print("Acurácia: %f" % evaluator.evaluate(previsao_treino, {evaluator.metricName: "accuracy", evaluator.weightCol:'ColWeight'}))
  print("Precisão: %f" % evaluator.evaluate(previsao_treino, {evaluator.metricName: "weightedPrecision", evaluator.weightCol:'ColWeight'}))
  print("Recall: %f" % evaluator.evaluate(previsao_treino, {evaluator.metricName: "weightedRecall", evaluator.weightCol:'ColWeight'}))
  print("F1: %f" % evaluator.evaluate(previsao_treino, {evaluator.metricName: "weightedFMeasure", evaluator.weightCol:'ColWeight' }))
  print('='*50)
  print('-'*50)
  print('')
  print('Dados de Teste')
  print('-'*50)
  print("Acurácia: %f" % evaluator.evaluate(previsao_teste, {evaluator.metricName: "accuracy", evaluator.weightCol:'ColWeight'}))
  print("Precisão: %f" % evaluator.evaluate(previsao_teste, {evaluator.metricName: "weightedPrecision", evaluator.weightCol:'ColWeight'}))
  print("Recall: %f" % evaluator.evaluate(previsao_teste, {evaluator.metricName: "weightedRecall", evaluator.weightCol:'ColWeight'}))
  print("F1: %f" % evaluator.evaluate(previsao_teste, {evaluator.metricName: "weightedFMeasure", evaluator.weightCol:'ColWeight' }))
  print('-'*50)
  print('')
  print('Matrix de Confusão')
  print('-'*50)
  calcula_mostra_matriz_confusao(previsao_teste, normalize=False)
  print('='*50)

"""## LogisticRegression"""

lr = LogisticRegression(weightCol='ColWeight')

modelo_lr = lr.fit(train)

previsao_modelo_lr_train = modelo_lr.transform(train)
previsao_modelo_lr_test = modelo_lr.transform(test)

previsao_modelo_lr_train.show()

evaluator = MulticlassClassificationEvaluator()

calcula_metricas('Pontuação LogisticRegression', previsao_modelo_lr_train, previsao_modelo_lr_test)

"""## DecisionTreeClassifier"""

dtc = DecisionTreeClassifier(seed=SEED, weightCol='ColWeight')

modelo_dtc = dtc.fit(train)

previsoes_modelo_dtc_train = modelo_dtc.transform(train)
previsoes_modelo_dtc_test = modelo_dtc.transform(test)

calcula_metricas('Pontuação DecisionTreeClassifier', previsoes_modelo_dtc_train, previsoes_modelo_dtc_test)

"""## Random Forest Classifier"""

rfc = RandomForestClassifier(seed=SEED, weightCol='ColWeight')

modelo_rfc = rfc.fit(train)

previsao_modelo_rfc_train = modelo_rfc.transform(train)
previsao_modelo_rfc_test = modelo_rfc.transform(test)

calcula_metricas('Pontuação RandomForestClassifier', previsao_modelo_rfc_train, previsao_modelo_rfc_test)

"""# **Cross Validate**

## Conhecendo os Parâmetros e fazendo o Cross Validator
"""

print(modelo_lr.explainParam('maxIter'))
print(modelo_lr.explainParam('aggregationDepth'))
print(modelo_lr.explainParam('family'))
print(modelo_lr.explainParam('threshold'))
print(modelo_lr.explainParam('fitIntercept'))
print(modelo_lr.explainParam('elasticNetParam'))
print(modelo_lr.explainParam('regParam'))

"""
Criando o Estimador
"""
lr = LogisticRegression(weightCol='ColWeight')

"""
Criando o mapa de parâmetros
"""
paramns = ParamGridBuilder()\
.addGrid(lr.maxIter, [100, 200, 300])\
.addGrid(lr.aggregationDepth, [2, 4, 6])\
.addGrid(lr.elasticNetParam, [0, 1])\
.build()

"""
Após criar o ParamMaps, crio o evaluator, coloquei o 
ColWeight devido ao desbalanceamento dos dados, além disso 
os dados no momento de treinar os dados passei o ColWeight
"""
evaluator = MulticlassClassificationEvaluator(weightCol='ColWeight')

"""
Então crio o CrossValidator, passando o estimador LogisticRegression,
o mapa de parâmetros, o evaluator, peço para quebrar em 5 partes, e passo a seed
que defini no começo do notebook, 'SEED=101'
"""
cv = CrossValidator(estimator=lr, 
               estimatorParamMaps=paramns,
               evaluator = evaluator,
               numFolds=5,
               seed=SEED)

modelo_final = cv.fit(train)

previsoes_modelo_final_treino = modelo_final.transform(train)
previsoes_modelo_final_teste = modelo_final.transform(test)

calcula_metricas('LogisticRegression com CrossValidator',
                 previsoes_modelo_final_treino,
                 previsoes_modelo_final_teste)

"""## Construindo o modelo final, e prevendo a satisfação do cliento no arquivo de teste"""

melhor_modelo = modelo_final.bestModel

print(f'O MaxIter foi {melhor_modelo.getMaxIter()}')
print(f'O aggregationDepth foi {melhor_modelo.getAggregationDepth()}')
print(f'O ElasticNetParam ideal foi {melhor_modelo.getElasticNetParam()}')

lr = LogisticRegression(weightCol='ColWeight', maxIter=200, aggregationDepth=2, elasticNetParam=0)
modelo = lr.fit(dados_preparados)

dados_teste = spark.read.csv('/content/drive/MyDrive/.Data Science/Projetos para Portfólio/Santander/Customer Satisfaction/data/test.csv', header=True, inferSchema=True)

dados_teste.show(5)

dados_teste_preparados = assembler.transform(dados_teste).select('features')

dados_teste_preparados.show()

previsoes = modelo.transform(dados_teste_preparados)

previsoes.show()